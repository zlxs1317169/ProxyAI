project.label=ProxyAI
notification.group.name=notification.group.name
notification.group.sticky.name=notification.group.sticky.name
action.generateCommitMessage.title=Generate Message
action.generateCommitMessage.description=Generate git commit message
action.generateCommitMessage.serviceWarning=Messages can only be generated with OpenAI, Custom OpenAI, or Azure service
action.generateCommitMessage.missingCredentials=Credentials not provided
action.includeFilesInContext.title=Include In Context...
action.includeFileInContext.title=Include File In Context...
action.includeFilesInContext.dialog.title=Include In Context
action.includeFilesInContext.dialog.description=Choose the files that you wish to include in the final prompt
action.includeFilesInContext.dialog.repeatableContext.label=Repeatable context:
action.includeFilesInContext.dialog.restoreToDefaults.label=Restore to Defaults
action.openSettings.title=Open Settings
action.openSettings.description=Open ProxyAI settings
action.statusbar.startServer.text=Start Server
action.statusbar.startServer.description=Start LLaMA Server
action.statusbar.startServer.MainMenu.text=Start Server
action.statusbar.stopServer.text=Stop Server
action.statusbar.stopServer.description=Stop LLaMA Server
action.statusbar.stopServer.MainMenu.text=Stop Server
action.statusbar.enableCompletions.text=Enable Completions
action.statusbar.enableCompletions.description=Enable Code Completions
action.statusbar.enableCompletions.MainMenu.text=Enable Completions
action.statusbar.disableCompletions.text=Disable Completions
action.statusbar.disableCompletions.description=Disable Code Completions
action.statusbar.disableCompletions.MainMenu.text=Disable Completions
action.statusbar.enableNextEdits.text=Enable Next Edits
action.statusbar.enableNextEdits.description=Enable Next Edits
action.statusbar.enableNextEdits.MainMenu.text=Enable Next Edits
action.statusbar.disableNextEdits.text=Disable Next Edits
action.statusbar.disableNextEdits.description=Disable Next Edits
action.statusbar.disableNextEdits.MainMenu.text=Disable Next Edits
action.compareWithOriginal.title=Compare with Original
action.applyDirectly.title=Auto Apply
action.explainGitCommit.title=Explain Commit with ProxyAI
action.explainGitCommit.description=Generate a detailed explanation of the commit changes using ProxyAI
settings.displayName=ProxyAI: Settings
settings.openaiQuotaExceeded=OpenAI quota exceeded.
settingsConfigurable.displayName.label=Display name:
settingsConfigurable.service.label=Selected provider:
settingsConfigurable.service.codeCompletion.label=Code completion provider:
settingsConfigurable.service.codegpt.apiKey.comment=You can find the API key in your <a href="https://tryproxy.io/account">User settings</a>.
settingsConfigurable.service.codegpt.chatCompletionModel.comment=Choose a model optimized for conversational interactions, including assistance with general queries and explanations.
settingsConfigurable.service.codegpt.codeCompletionModel.comment=Choose a model tailored for code completion-related tasks.
settingsConfigurable.service.codegpt.enableNextEdits.comment=If checked, ProxyAI will suggest multi-line changes as you type.
settingsConfigurable.service.codegpt.enableCodeCompletion.comment=If checked, ProxyAI will suggest changes as you type.
settingsConfigurable.service.custom.openai.apiKey.comment=A secret value stored in the system's Keychain or KeePass, depending on your OS. This approach is recommended over storing the secret in the header as plain text.
settingsConfigurable.service.custom.openai.apiKey.provider.name=Custom provider name:
settingsConfigurable.service.custom.openai.exportDialog.filename=File name:
settingsConfigurable.service.custom.openai.exportDialog.saveTo=Save to:
settingsConfigurable.service.custom.openai.exportDialog.title=Target File
settingsConfigurable.service.custom.openai.exportDialog.exportError=Error exporting OpenAI settings
settingsConfigurable.service.custom.openai.exportDialog.importError=Error importing OpenAI settings
settingsConfigurable.service.openai.apiKey.comment=You can find the API key in your <a href="https://platform.openai.com/account/api-keys">User settings</a>.
settingsConfigurable.service.openai.customModel.label=Custom model:
settingsConfigurable.service.openai.organization.label=Organization:
settingsConfigurable.section.openai.organization.comment=Useful when you are part of multiple organizations <sup><strong>optional</strong></sup>
settingsConfigurable.service.google.apiKey.comment=You can find the API key in your <a href="https://aistudio.google.com/app/apikey">User settings</a>.
settingsConfigurable.service.google.model.comment=Note: Gemini Vision models <a href="https://ai.google.dev/gemini-api/docs/get-started/web?multi-turn-conversations-chat&hl=en#multi-turn-conversations-chat">do not yet support chats</a>.
settingsConfigurable.service.anthropic.apiKey.comment=You can find the API key in your <a href="https://console.anthropic.com/settings/keys">User settings</a>.
settingsConfigurable.service.anthropic.apiVersion.comment=We always recommend using the <a href="https://docs.anthropic.com/claude/reference/versions">latest API version</a> whenever possible.
settingsConfigurable.service.anthropic.model.comment=For details on model comparison metrics, see <a href="https://docs.anthropic.com/claude/docs/models-overview#model-comparison">model comparison</a>.
settingsConfigurable.service.llama.modelPreferences.title=Model Preferences
settingsConfigurable.service.llama.serverPreferences.title=Server Preferences
settingsConfigurable.service.llama.modelSize.label=Model size:
settingsConfigurable.service.llama.quantization.label=Quantization:
settingsConfigurable.service.llama.quantization.comment=Quantization is a technique to reduce the computational and memory costs of running inference. <a href="https://huggingface.co/docs/optimum/concept_guides/quantization">Learn more</a>
settingsConfigurable.service.llama.customModelPath.label=Model path:
settingsConfigurable.service.llama.customModelPath.comment=Only .gguf files are supported
settingsConfigurable.service.llama.customServerPath.label=Server path:
settingsConfigurable.service.llama.customServerPath.comment=Precompiled executable llama-cpp server, only .exe (Windows) or executable File (Linux) are supported
settingsConfigurable.service.llama.promptTemplate.comment=Choose the template to use during interactions with the language model. Make sure it matches the custom model you're working with.
settingsConfigurable.service.llama.infillTemplate.comment=The template to use for code completions. Make sure the model you're working with supports code infilling.
settingsConfigurable.service.llama.downloadModelLink.label=Download Model
settingsConfigurable.service.llama.cancelDownloadLink.label=Cancel Downloading
settingsConfigurable.service.llama.linkToModel.label=Link to model
settingsConfigurable.service.llama.contextSize.label=Prompt context size:
settingsConfigurable.service.llama.contextSize.comment=The size of the prompt context. LLaMA models were built with a context of 2048, which will provide better results for longer input/inference.
settingsConfigurable.service.llama.threads.label=Threads:
settingsConfigurable.service.llama.threads.comment=The number of threads available to execute the model. It is not recommended to specify a number greater than the number of processor cores.
settingsConfigurable.service.llama.additionalParameters.label=Additional parameters:
settingsConfigurable.service.llama.additionalParameters.comment=<html>Additional command-line parameters for the server startup process, separated by commas. See the full <a href="https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md">list of options</a>.<p><i>Example: "--n-gpu-layers, 1,  --no-mmap, --mlock"</i></p></html>
settingsConfigurable.service.llama.additionalBuildParameters.label=Additional build parameters:
settingsConfigurable.service.llama.additionalBuildParameters.comment=<html>Additional command-line parameters for the server build process, separated by commas. See the full <a href="https://github.com/ggerganov/llama.cpp/tree/master?tab=readme-ov-file#build">list of build options</a>.<p><i>Example: "LLAMA_CUDA=1,CUDA_DOCKER_ARCH=all"</i></p></html>
settingsConfigurable.service.llama.additionalEnvironmentVariables.label=Additional environment variables:
settingsConfigurable.service.llama.additionalEnvironmentVariables.comment=<html>Additional environment variables for the server build and run process, separated by whitespaces. Can be used to e.g. set CUDA variables (see the full <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars">list of env vars</a>)<p><i>Example: "CUDA_VISIBLE_DEVICES=0,1"</i></p></html>
settingsConfigurable.service.llama.baseHost.label=Base host:
settingsConfigurable.service.llama.baseHost.comment=URL to existing LLama server
settingsConfigurable.service.llama.startServer.label=Start server
settingsConfigurable.service.llama.startServer.opposite=Stop
settingsConfigurable.service.llama.stopServer.label=Stop server
settingsConfigurable.service.llama.stopServer.opposite=Start
settingsConfigurable.service.llama.progress.serverRunning=Server running
settingsConfigurable.service.llama.progress.serverStopped=Server stopped
settingsConfigurable.service.llama.progress.stoppingServer=Stopping server...
settingsConfigurable.service.llama.progress.startingServer=Starting server...
settingsConfigurable.service.llama.progress.downloadingModel.title=Downloading Model
settingsConfigurable.service.llama.progress.downloadingModelIndicator.text=Downloading %s...
settingsConfigurable.service.llama.overlay.modelNotDownloaded.text=Model is not downloaded
settingsConfigurable.shared.authentication.title=Authentication
settingsConfigurable.shared.requestConfiguration.title=Request Configuration
settingsConfigurable.shared.apiKey.label=API key:
settingsConfigurable.shared.apiKey.comment=API Key for authentication, added to 'Authorization' header as bearer (Optional)
settingsConfigurable.shared.baseHost.label=Base host:
settingsConfigurable.shared.path.label=Path:
settingsConfigurable.shared.model.label=Model:
configurationConfigurable.displayName=ProxyAI: Configuration
configurationConfigurable.table.title=Editor Actions
configurationConfigurable.table.emptyText=No actions configured
configurationConfigurable.table.header.actionColumnLabel=Action
configurationConfigurable.table.header.promptColumnLabel=Prompt
configurationConfigurable.table.action.revertToDefaults.text=Revert to Defaults
configurationConfigurable.table.action.addKeymap.text=Add Shortcut
configurationConfigurable.checkForPluginUpdates.label=Check for plugin updates automatically
configurationConfigurable.checkForNewScreenshots.label=Check for new screenshots automatically
configurationConfigurable.openNewTabCheckBox.label=Open a new chat on each action
configurationConfigurable.enableMethodNameGeneration.label=Enable method name lookup suggestions
configurationConfigurable.autoFormatting.label=Enable automatic code formatting
configurationConfigurable.autocompletionPostProcessing.label=Enable code completion post processing
configurationConfigurable.autocompletionContextAwareCheckBox.label=Enable project context aware code completion
configurationConfigurable.autocompletionGitContextCheckBox.label=Enable Git context for code completions
configurationConfigurable.section.assistant.title=Assistant Configuration
configurationConfigurable.section.assistant.systemPromptField.label=System prompt:
configurationConfigurable.section.assistant.systemPromptField.comment=The system message helps to set the behaviour of the assistant
configurationConfigurable.section.assistant.temperatureField.label=Temperature:
configurationConfigurable.section.assistant.temperatureField.comment=The value of randomness. Must be between 0 and 1
configurationConfigurable.section.assistant.maxTokensField.label=Max completion tokens:
configurationConfigurable.section.assistant.maxTokensField.comment=The maximum capacity for completion.
configurationConfigurable.section.assistant.llamacppParams.title=Configuration Options for llama.cpp
configurationConfigurable.section.codeCompletion.title=Code Completion
configurationConfigurable.section.codeCompletion.multiLineCompletions.description=If checked, the completion will be able to span multiple lines.
configurationConfigurable.section.codeCompletion.postProcess.title=Enable tree-sitter post-processing
configurationConfigurable.section.codeCompletion.postProcess.description=If checked, the completion will be post-processed using the tree-sitter parser.
configurationConfigurable.section.codeCompletion.gitDiff.title=Enable git diff context
configurationConfigurable.section.codeCompletion.collectDependencyStructure.title=Enable dependency analyzer
configurationConfigurable.section.codeCompletion.analyzeDepth.title=Code analyze depth:
configurationConfigurable.section.codeCompletion.analyzeDepth.comment=The parameter limits the depth of the PSI structure traversal. Currently, it is implemented only for the Kotlin language.
configurationConfigurable.section.codeCompletion.collectDependencyStructure.description=Enabling the setting allows the plugin to collect the dependency structure, which increases the accuracy of the proposed data, but consumes more tokens per request. Currently, it is implemented only for the Kotlin language.
configurationConfigurable.section.codeCompletion.gitDiff.description=If checked, the user's most recent unstaged git diff will be included when requesting completion.
configurationConfigurable.section.chatCompletion.title=Chat Completion
configurationConfigurable.section.chatCompletion.retryOnFailedDiffSearch.title=Enable retry on failed diff search
configurationConfigurable.section.chatCompletion.retryOnFailedDiffSearch.description=If checked, the plugin will retry the diff search if it fails.
configurationConfigurable.section.chatCompletion.editorContextTag.title=Enable automatic file tagging
configurationConfigurable.section.chatCompletion.editorContextTag.description=If enabled, the content from open editor files will be automatically included with each message you send.
configurationConfigurable.section.chatCompletion.psiStructure.title=Enable dependency structure analysis of attached files.
configurationConfigurable.section.chatCompletion.psiStructure.analyzeDepth.title=Code analyze depth:
configurationConfigurable.section.chatCompletion.psiStructure.analyzeDepth.comment=The parameter limits the depth of the PSI structure traversal. Currently, it is implemented only for the Kotlin language.
configurationConfigurable.section.chatCompletion.psiStructure.description=If enabled, the class structure that is present in the imports of the attached files will be added in the context of the dialog. A structure refers to the source code in files that include constructors, fields, and methods, with all modifiers, arguments, and return types, but without an implementation. The implementation of dependencies is intentionally excluded in order to find a balance between a high-quality chat context and saving tokens.
settingsConfigurable.service.llama.predefinedModel.comment=Download and use vetted models from HuggingFace.
settingsConfigurable.service.llama.customModel.comment=Use your own GGUF model file from a local path on your computer.
settingsConfigurable.service.custom.openai.testConnection.label=Test Connection
settingsConfigurable.service.custom.openai.presetTemplate.label=Preset template:
settingsConfigurable.service.custom.openai.url.label=URL:
settingsConfigurable.service.custom.openai.linkToDocs=Link to API docs
settingsConfigurable.service.custom.openai.connectionSuccess=Connection successful.
settingsConfigurable.service.custom.openai.connectionFailed=Connection failed.
settingsConfigurable.service.custom.openai.importSettings=Import settings...
settingsConfigurable.service.custom.openai.exportSettings=Export settings
settingsConfigurable.prompts.import=Import settings...
settingsConfigurable.prompts.export=Export settings
settingsConfigurable.prompts.exportDialog.saveTo=Save to:
settingsConfigurable.prompts.exportDialog.exportError=Error exporting prompts settings
settingsConfigurable.prompts.exportDialog.title=Target File
settingsConfigurable.prompts.importDialog.importError=Error importing prompts settings
settingsConfigurable.service.ollama.models.refresh=Refresh Models
settingsConfigurable.service.ollama.codeCompletionModel.label=Model for code completion:
advancedSettingsConfigurable.displayName=ProxyAI: Advanced Settings
advancedSettingsConfigurable.proxy.title=HTTP/SOCKS Proxy
advancedSettingsConfigurable.proxy.typeComboBoxField.label=Proxy:
advancedSettingsConfigurable.proxy.hostField.label=Host name:
advancedSettingsConfigurable.proxy.authCheckBoxField.label=Proxy authentication
advancedSettingsConfigurable.proxy.usernameField.label=Username:
advancedSettingsConfigurable.proxy.passwordField.label=Password:
advancedSettingsConfigurable.connectionSettings.title=Connection Settings
advancedSettingsConfigurable.connectionSettings.connectionTimeout.label=Connection timeout (s):
advancedSettingsConfigurable.connectionSettings.readTimeout.label=Read timeout (s):
codebaseIndexing.task.title=Indexing codebase
dialog.deleteConversation.title=Delete Conversation
dialog.deleteConversation.description=Are you sure you want to delete this conversation?
dialog.tokenLimitExceeded.title=Token Limit Exceeded
dialog.tokenLimitExceeded.description=The maximum default token limit has been reached. Do you want to proceed with the conversation despite the higher messaging cost?
dialog.tokenSoftLimitExceeded.title=Soft Limit Exceeded
dialog.tokenSoftLimitExceeded.description=Warning: The 'git diff' output contains %d tokens, indicating a substantial amount of changes. Are you sure you want to continue?
dialog.continue=Continue
editor.diff.title=ProxyAI Diff
editor.diff.local.content.title=ProxyAI suggested code
toolwindow.chat.editor.action.copy.description=Copy generated code
toolwindow.chat.editor.action.autoApply.title=Auto Apply
toolwindow.chat.editor.action.autoApply.disabledTitle=Auto apply is only available with ProxyAI provider
toolwindow.chat.editor.action.autoApply.description=Apply suggested changes automatically
toolwindow.chat.editor.action.autoApply.noActiveFile=Active file not found
toolwindow.chat.editor.action.autoApply.fileTooLarge=Active file too large to process
toolwindow.chat.editor.action.autoApply.reject=Reject All
toolwindow.chat.editor.diff.reading=Reading...
toolwindow.chat.editor.diff.thinking=Thinking...
toolwindow.chat.editor.diff.editing=Editing...
toolwindow.chat.editor.diff.retrying=Retrying...
toolwindow.chat.editor.action.autoApply.error=Something went wrong while applying changes. {0}
toolwindow.chat.editor.action.autoApply.taskTitle=Apply changes
toolwindow.chat.editor.action.autoApply.loadingMessage=ProxyAI: Applying changes
toolwindow.chat.editor.action.autoApply.successMessage=Changes successfully applied to the file.
diff.acceptedPanel.revertChanges=Revert Changes
diff.acceptedPanel.viewDetails=View Details
diff.acceptedPanel.before=Before
diff.acceptedPanel.after=After
toolwindow.chat.editor.action.diff.description=Diff editor code against the generated one
toolwindow.chat.editor.action.edit.title=Edit Source
toolwindow.chat.editor.action.disableEditing.title=Disable Editing
toolwindow.chat.editor.action.edit.description=Edit generated code
toolwindow.chat.editor.action.newFile.title=New File
toolwindow.chat.editor.action.newFile.description=Create new file from generated code
toolwindow.chat.editor.action.replaceSelection.title=Replace Selection
toolwindow.chat.editor.action.replaceSelection.description=Replace main editor selected code
toolwindow.chat.editor.action.insertAtCaret.title=Insert at Caret
toolwindow.chat.editor.action.insertAtCaret.description=Insert generated code after main editor caret position
toolwindow.chat.editor.action.expand=Show More
toolwindow.chat.editor.action.collapse=Show Less
toolwindow.chat.response.action.reloadResponse.text=Reload Response
toolwindow.chat.response.action.reloadResponse.description=Reload response description
toolwindow.chat.response.action.deleteResponse.text=Delete Response
toolwindow.chat.response.action.deleteResponse.description=Delete response description
toolwindow.chat.youProCheckBox.text=Use GPT-4 model
toolwindow.chat.youProCheckBox.enable=Turn on for complex queries
toolwindow.chat.youProCheckBox.disable=Turn off for faster responses
toolwindow.chat.youProCheckBox.notAllowed=Enable by subscribing to YouPro plan
toolwindow.chat.textArea.emptyText=Ask anything... Use '@' to include additional context
service.codegpt.title=ProxyAI
service.openai.title=OpenAI
service.custom.openai.title=Custom OpenAI
service.anthropic.title=Anthropic
service.azure.title=Azure
service.google.title=Google
service.llama.title=LLaMA C/C++ (Local)
service.ollama.title=Ollama (Local)
validation.error.model.notExists='%s' is not available, please select another model
validation.error.fieldRequired=This field is required.
validation.error.invalidEmail=The email you entered is invalid.
validation.error.mustBeNumber=Value must be number.
validation.error.mustBeBetweenZeroAndOne=Value must be between 0 and 1.
validation.error.mustBeGreaterThanZero=Value must be greater than 0
checkForUpdatesTask.title=Checking for ProxyAI update...
checkForUpdatesTask.notification.message=An update for ProxyAI is available.
checkForUpdatesTask.notification.installButton=Install update
llamaServerAgent.buildingProject.description=Building server...
llamaServerAgent.serverBootup.description=Booting up server...
notification.compilationError.description=ProxyAI has detected a compilation error. Would you like assistance in resolving it?
notification.compilationError.okLabel=Resolve errors
notification.completionError.description=Completion failed:<br/>%s
statusBar.widget.tooltip=ProxyAI: Status
shared.acceptAll=Accept All
shared.promptTemplate=Prompt template:
shared.infillPromptTemplate=Infill template:
shared.apiVersion=API version:
shared.escToCancel=Esc to cancel
shared.cancel=Cancel
shared.confirm=Confirm
shared.copy=Copy
shared.copyCode=Copy Code
shared.copyMessageContents=Copy Message Contents
shared.copyToClipboard=Copy to clipboard
shared.copiedToClipboard=Copied to clipboard
shared.configuration=Configuration
shared.delete=Delete Message
shared.deleteDescription=Delete message
shared.reload=Reload Message
shared.reloadDescription=Reload message
shared.port=Port:
shared.discard=Discard
shared.notification.doNotShowAgain=Do not show again
shared.loading=Loading...
shared.website=Website
codeCompletion.progress.title=Code completion in progress
imageAttachmentNotification.content=New image detected on desktop. Would you like to attach it to your current conversation?
imageAttachmentNotification.action=Attach image
action.attachImage=Attach Image
action.attachImageDescription=Attach an image
imageFileChooser.title=Select Image
imageAccordion.title=Attached image
shared.image=Image
shared.chatCompletions=Chat Completions
shared.codeCompletions=Code Completions
codeCompletionsForm.enableFeatureText=Enable code completions
codeCompletionsForm.parseResponseAsChatCompletions=Parse response as Chat Completions
codeCompletionsForm.overrideFimTemplate.label=Use built-in FIM template
codeCompletionsForm.overrideFimTemplate.description=If checked, ProxyAI will apply the corresponding FIM template for the selected model.
codeCompletionsForm.selectFimTemplate=FIM template:
codeCompletionsForm.maxTokensLabel=Max tokens:
codeCompletionsForm.maxTokensComment=The maximum number of tokens that will be generated in the code completion.
editCodePopover.title=Edit Code
editCodePopover.textField.emptyText=Editing instructions...
editCodePopover.textField.followUp.emptyText=Ask a follow-up question
editCodePopover.textField.comment=Provide instructions for the code modification.
editCodePopover.submitButton.title=Submit Edit
editCodePopover.acceptButton.title=Accept Suggestion
editCodePopover.followUpButton.title=Submit Follow-up
smartTextPane.submitButton.title=Send Message
smartTextPane.submitButton.description=Send message
smartTextPane.stopButton.title=Stop
smartTextPane.stopButton.description=Stop completion
chatMessageResponseBody.webPages.title=WEB PAGES
chatMessageResponseBody.webDocs.startProgress.label=Analyzing web content...
addDocumentation.popup.title=Add Documentation
addDocumentation.popup.form.name.label=Name:
addDocumentation.popup.form.url.label=URL:
addDocumentation.popup.form.url.comment=Enter the full web address of the documentation.
addDocumentation.popup.form.saveCheckbox.label=Save for future reference
userMessagePanel.documentation.title=DOCUMENTATION
userMessagePanel.persona.title=PERSONA
suggestionGroupItem.files.displayName=Files
suggestionGroupItem.folders.displayName=Folders
suggestionGroupItem.personas.displayName=Personas
suggestionGroupItem.history.displayName=History
suggestionGroupItem.docs.displayName=Docs
suggestionGroupItem.git.displayName=Git
suggestionGroupItem.mcp.displayName=MCP (soon)
suggestionGroupItem.codeAnalyze.displayName=Code Analyze
suggestionActionItem.attachImage.displayName=Image
suggestionActionItem.attachImage.description=Select an image file to attach
suggestionActionItem.webSearch.displayName=Web
suggestionActionItem.viewDocumentations.displayName=View all docs
suggestionActionItem.createPersona.displayName=Add new persona
suggestionActionItem.createDocumentation.displayName=Add new doc
suggestionActionItem.includeOpenFiles.displayName=Include Open Files
suggestionActionItem.includeCurrentChanges.displayName=Include Current Changes
tagPopupMenuItem.close=Close
tagPopupMenuItem.closeOthers=Close Other Tags
tagPopupMenuItem.closeAll=Close All Tags
tagPopupMenuItem.closeTagsToLeft=Close Tags to the Left
tagPopupMenuItem.closeTagsToRight=Close Tags to the Right
toolwindow.chat.loading=Generating response...
headerPanel.error.searchBlockNotMapped.title=Failed to Locate Search Block
llama.build.cmake.setup=Setting up CMake...
llama.build.cmake.build=Building project...
llama.build.startingBuild=Starting Llama Server Build
llama.build.phase.setup=Phase 1: CMake Setup
llama.build.phase.build=Phase 2: Building Project
llama.build.phase.setupFailed=CMake setup failed
llama.build.phase.buildFailed=Build failed
llama.build.cache.cleanup=Detected CMake cache path mismatch, cleaning up build directory...
llama.server.buildStopped=Build stopped by user
llama.server.starting=Starting server...
llama.server.running=Server running successfully
llama.server.startupFailed=Server startup failed
llama.server.stopping.cmake=Stopping CMake setup process
llama.server.stopping.build=Stopping build process
llama.error.server.startup=Unable to start llama server:\n{0}
llama.error.server.startupWithDetails=Server startup failed: {0}
llama.ui.tab.serverConfiguration=Server Configuration
llama.ui.tab.serverLogs=Server Logs
llama.ui.tab.buildOutput=Build Output
llama.ui.button.stopBuild=Stop Build
llama.ui.status.running=Server status: Running
llama.ui.status.building=Server status: Building...
llama.ui.status.stopped=Server status: Stopped
llama.ui.status.prefix=Server status: {0}
llama.ui.action.clear=Clear
llama.ui.action.clear.description=Clear console
llama.ui.action.scrollToEnd=Scroll to End
llama.ui.action.scrollToEnd.description=Scroll to bottom
llama.process.startingBuild=Starting server build process...
llama.debug.buildLoggingStrategy=DEBUG: Build logging strategy initialized
